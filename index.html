<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Inside Project Nordâ€‰â€“â€‰SNN Architecture Walkthrough</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,400;0,600;0,700;1,400&family=IBM+Plex+Sans:wght@400;500;600;700;800&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <header>
      <div class="container">
        <div class="tag">
          Project Nord Â· 144M SNN Language Model Â· Built from Scratch
        </div>
        <h1>Inside the Spiking Engine</h1>
        <p style="font-size: 1.2rem; color: var(--text-main); max-width: 680px">
          Standard AI models perform dense, expensive matrix multiplications for
          every single token. Biological brains don'tâ€‰â€“â€‰they use sparse,
          <strong>1-bit binary spikes</strong>, firing only 2â€“3% of the time.
          This page traces one word through Nord's entire computational
          pipeline, step by step, with every animation mathematically faithful
          to the running source code.
        </p>
        <p style="max-width: 660px">
          <strong>Project Nord</strong> solves five problems that every prior
          SNN language model either failed at or avoided entirelyâ€‰â€“â€‰all in one
          novel, from-scratch architecture. No distillation, no teacher
          transformer. Let's trace the journey.
        </p>
      </div>
    </header>

    <main class="container">
      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• STEP 1: ENCODER â•â•â•â• -->
      <div class="card">
        <div class="step-badge">Step 1 Â· Time Expansion</div>
        <h2>The Multi-Scale Temporal Encoder</h2>
        <p>
          When you type <em>"Hello"</em>, a standard transformer turns it into a
          single 512-dimensional float vector and stops there. A spiking network
          can't process a frozen static frameâ€‰â€“â€‰it requires a
          <strong>stream of input current flowing across time</strong>.
        </p>
        <p>
          Nord's <code>TemporalSpikeEncoder</code> solves this by expanding that
          one static vector into
          <strong>10 discrete timesteps of injected current</strong>, split
          across two functional scales:
        </p>
        <ul>
          <li>
            <strong class="hl-teal">T_fast (8 steps, scale = 15.0):</strong>
            High-amplitude, rapidly-varying drive. Each step uses a different
            learned sigmoid gate over the 512 dims, making it volatileâ€‰â€“â€‰ideal
            for capturing local morpheme-level details.
          </li>
          <li>
            <strong class="hl-amber">T_slow (2 steps, scale = 5.0):</strong>
            Gently suppressed, stable drive. Lower amplitude anchors the network
            to a broader "context summary" without overpowering the fast path.
          </li>
        </ul>

        <div class="code-window">
          <div class="code-header">
            <div class="dot"></div>
            <div class="dot"></div>
            <div class="dot"></div>
            <span class="code-fname"
              >nord_core.py Â· TemporalSpikeEncoder.forward()</span
            >
          </div>
          <div class="code-body">
            <pre><code><span class="c-c"># Token embedding + learned temporal projection</span>
x = temporal_proj(embed(token_ids))     <span class="c-c"># (BÃ—S, D=512)</span>

<span class="c-c"># Fast path: T=8 steps, each with its own learned gate per dimension</span>
fast_gates = torch.<span class="c-f">sigmoid</span>(fast_basis)   <span class="c-c"># (8, 512)â€‰â€“â€‰learned per-step gate</span>
fast = fast_gates * x * drive_scale      <span class="c-c"># drive_scale â‰ˆ 15.0</span>

<span class="c-c"># Slow path: T_slow=2 steps, weaker amplitude</span>
slow_gates = torch.<span class="c-f">sigmoid</span>(slow_basis)   <span class="c-c"># (2, 512)â€‰â€“â€‰different learned gate</span>
slow = slow_gates * x * slow_scale       <span class="c-c"># slow_scale â‰ˆ 5.0</span>

<span class="c-c"># Concatenate â†’ (10, BÃ—S, 512) injected current tensor</span>
current_in = torch.<span class="c-f">cat</span>([fast, slow], dim=<span class="c-n">0</span>)</code></pre>
          </div>
        </div>

        <div class="vis-wrap">
          <canvas id="encCanvas"></canvas>
          <div class="vis-caption">
            The static word embedding (bottom bar) is gated and scaled into 10
            temporal current streams. Teal bars (T1â€“T8, fast) oscillate with
            high amplitude (~15Ã—). Amber bars (T9â€“T10, slow) move gently at low
            amplitude (~5Ã—). Each bar's height represents injected current for
            that timestep.
          </div>
        </div>
      </div>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• STEP 2: LIF â•â•â•â• -->
      <div class="card">
        <div class="step-badge">Step 2 Â· Binary Conversion</div>
        <h2>The AssociativeLIF Neuronâ€‰â€“â€‰Integrate, Fire, Reset</h2>
        <p>
          We now have 10 timesteps of continuous current. The
          <code>AssociativeLIF</code> (Leaky Integrate-and-Fire) block converts
          this stream into <strong>1-bit binary spikes</strong>, achieving the
          97% sparsity that makes Nord energy-efficient.
        </p>
        <p>
          Each timestep runs three coupled equations: the
          <strong>synaptic current</strong> (<code>i_syn</code>) integrates
          incoming drive with exponential decay (<code>tau_syn = 0.5</code>),
          the <strong>membrane voltage</strong> (<code>v_mem</code>) integrates
          that current with leak (<code>tau_mem = 0.9</code>), and if voltage
          crosses <code>v_threshold = 0.25</code> a spike fires. The neuron then
          enters a hard <strong>refractory period</strong> of 2 timesteps where
          voltage is clamped to <code>v_reset = -0.1</code>, preventing any
          re-firing.
        </p>
        <p>
          After each spike, <strong>Cascade amplification</strong> (Step 3)
          injects a sub-threshold ripple back into <code>i_syn</code> to keep
          gradient flow alive.
        </p>

        <div class="code-window">
          <div class="code-header">
            <div class="dot"></div>
            <div class="dot"></div>
            <div class="dot"></div>
            <span class="code-fname"
              >nord_core.py Â· AssociativeLIF.forward()â€‰â€“â€‰per-timestep loop</span
            >
          </div>
          <div class="code-body">
            <pre><code><span class="c-k">for</span> t <span class="c-k">in</span> <span class="c-f">range</span>(T_total):           <span class="c-c"># T_total = 10</span>
    <span class="c-c"># 1. Synaptic integration (low-pass filter on input)</span>
    i_syn = beta_syn * i_syn + current_in[t]     <span class="c-c"># beta_syn â‰ˆ 0.50</span>

    <span class="c-c"># 2. Membrane voltage: leak or clamp during refractory</span>
    <span class="c-k">if</span> refractory:
        v_mem = v_reset   <span class="c-c"># = -0.1, hard clamp</span>
    <span class="c-k">else</span>:
        v_mem = beta_mem * v_mem + (<span class="c-n">1</span>-beta_mem) * i_syn   <span class="c-c"># beta_mem â‰ˆ 0.90</span>

    <span class="c-c"># 3. Fire if above threshold (ATan surrogate gradient for backprop)</span>
    spike = spike_fn(v_mem, threshold)           <span class="c-c"># â†’ 0 or 1</span>

    <span class="c-c"># 4. Soft reset + cascade ripple injection</span>
    v_mem = v_mem - spike * threshold            <span class="c-c"># soft reset (â‰ˆ 0)</span>
    <span class="c-k">if</span> spike.sum() > <span class="c-n">0</span>:
        i_syn += cascade_amplify(spike)          <span class="c-c"># neighbor ripple (Step 3)</span>
    refrac_counter = <span class="c-f">set_refractory</span>(spike, <span class="c-n">2</span>)  <span class="c-c"># 2-step lock</span></code></pre>
          </div>
        </div>

        <div class="vis-wrap">
          <canvas id="lifCanvas"></canvas>
          <div class="vis-caption">
            Oscilloscope trace of a single LIF neuron.
            <strong>Bottom (Green)</strong> = filtered synaptic current
            <code>i_syn</code>. <strong>Middle (Teal)</strong> = membrane
            voltage <code>v_mem</code>. Notice the sub-threshold cascade
            injection (amber): it raises <code>v_mem</code> slightly but doesn't
            trigger a spike, keeping gradients alive. When the main burst pushes
            <code>v_mem</code> over the <strong>threshold 0.25</strong>,
            <strong>Top (Coral) spikes</strong> fire, and the neuron enters a
            2-step refractory clamp at âˆ’0.1.
          </div>
        </div>
      </div>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• STEP 3: CASCADE â•â•â•â• -->
      <div class="card">
        <div class="step-badge">Step 3 Â· Preventing Death</div>
        <h2>The Associative Cascadeâ€‰â€“â€‰Keeping Gradients Alive</h2>
        <p>
          Here's the central paradox of deep spiking networks: efficiency
          demands silence, but silence kills learning. At 97% sparsity,
          backpropagation gradients multiply through seas of zeros and
          <strong>vanish before reaching early layers</strong>. Nord's
          Associative Cascade solves this.
        </p>
        <p>
          The D=512 neurons are partitioned into
          <strong>64 topological clusters</strong> arranged in a ring. When
          neurons in a cluster fire, they send a "soft ripple" of sub-threshold
          current to their neighboring clusters via a learnable
          <code>64Ã—64 weight matrix</code>. Closer neighbors (within radius=3)
          receive stronger initial weights. This continuous, differentiable
          current keeps gradients flowing even when spikes themselves are binary
          zeros.
        </p>
        <p>
          Crucially, the neighbor weights and per-cluster gains are
          <strong>learned during training</strong>
          â€‰â€“â€‰the network decides which cluster topologies are most useful.
          <em
            >(Notice that the cascade also reverberates back into the
            originating cluster! While the neuron that just fired is protected
            by a hard refractory period, its silent neighbors in the same
            cluster receive the sub-threshold ripple, priming them for
            subsequent timesteps.)</em
          >
        </p>

        <div class="code-window">
          <div class="code-header">
            <div class="dot"></div>
            <div class="dot"></div>
            <div class="dot"></div>
            <span class="code-fname"
              >nord_core.py Â· AssociativeLIF._cascade_amplify()</span
            >
          </div>
          <div class="code-body">
            <pre><code><span class="c-c"># Group D=512 neurons into nc=64 clusters</span>
cluster_fire = scatter_add(spikes, cluster_ids) / (D // nc)  <span class="c-c"># (B, 64)</span>

<span class="c-c"># Learned soft neighbor weight matrixâ€‰â€“â€‰sigmoid keeps weights in (0,1)</span>
W = torch.<span class="c-f">sigmoid</span>(neighbor_weights)             <span class="c-c"># (64, 64)</span>

<span class="c-c"># Each cluster receives weighted sum of neighbors' spike rates</span>
neighbor_signal = (W @ cluster_fire.T).T          <span class="c-c"># (B, 64)</span>

<span class="c-c"># Per-cluster learnable gain (not one global scalar)</span>
neighbor_signal = neighbor_signal * cluster_gain   <span class="c-c"># (B, 64)</span>

<span class="c-c"># Scatter back to neurons by cluster membership â†’ added to i_syn</span>
<span class="c-k">return</span> neighbor_signal.gather(<span class="c-n">1</span>, cluster_ids)     <span class="c-c"># (B, D=512)</span></code></pre>
          </div>
        </div>

        <div class="vis-wrap">
          <div class="hint-tag">ğŸ–± Hover to trigger spikes</div>
          <canvas id="casCanvas"></canvas>
          <div class="vis-caption">
            <strong>Interactive:</strong> How does a 512-dimensional tensor use
            a flat ring? Through <em>Scatter-Gather</em>. The
            <strong>Outer Ring</strong> represents the 512 individual neurons in
            a layer. The <strong>Inner Ring</strong> represents the 64
            topological clusters. Hover to trigger an outer neuron (coral).
            Watch it send its spike down to its cluster
            (<code>scatter_add</code>). The cluster sends lateral ripples along
            the inner ring (<code>W matrix</code>). Neighboring clusters receive
            the ripple and push the signal back out to their constituent neurons
            (<code>gather</code>), raising them into a teal sub-threshold ghost
            state.
          </div>
        </div>
      </div>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• STEP 4: RESONANCE â•â•â•â• -->
      <div class="card">
        <div class="step-badge">Step 4 Â· Attention Replacement</div>
        <h2>Sparse Spiking Synaptic Resonance</h2>
        <p>
          Standard Transformers use Self-Attention: an expensive O(SÂ²) operation
          that computes a similarity score between every pair of tokens using
          continuous float vectors. Nord replaces this entirely with
          <strong>Spiking Synaptic Resonance</strong>.
        </p>
        <p>
          Query and Key projections are each passed through their own LIF
          neurons, producing <strong>binary spike patterns</strong> across T=10
          timesteps. The "resonance score" between position <em>i</em> (query)
          and position <em>j</em> (key) is then the dot product of their spike
          patternsâ€‰â€“â€‰how many times did they fire simultaneously across the 10
          temporal frames?
        </p>
        <p>
          To save memory, only the
          <strong>Top-K=64 resonance scores</strong> per query position are
          kept. All others are zeroed out before softmax, making attention 87.5%
          sparse for typical sequence lengths.
        </p>

        <div class="code-window">
          <div class="code-header">
            <div class="dot"></div>
            <div class="dot"></div>
            <div class="dot"></div>
            <span class="code-fname"
              >nord_core.py Â· SpikingSynapticResonance.forward()</span
            >
          </div>
          <div class="code-body">
            <pre><code><span class="c-c"># Project to Q, K, Vâ€‰â€“â€‰then spike Q and K independently</span>
q_spikes, _ = lif_q(W_q(x))  <span class="c-c"># (T=10, BÃ—S, D) binary spikes</span>
k_spikes, _ = lif_k(W_k(x))  <span class="c-c"># (T=10, BÃ—S, D) binary spikes</span>

<span class="c-c"># Flatten timeÃ—head-dim â†’ resonance = co-firing dot product</span>
q_flat = q_spikes.reshape(B, n_heads, S, T * d_head)   <span class="c-c"># (B, H, S, TÃ—Dh)</span>
k_flat = k_spikes.reshape(B, n_heads, S, T * d_head)
resonance = q_flat @ k_flat.transpose(<span class="c-n">-2</span>, <span class="c-n">-1</span>)          <span class="c-c"># (B, H, S, S)</span>

<span class="c-c"># Causal mask (no peeking at future tokens)</span>
resonance.masked_fill_(future_mask, float(<span class="c-s">"-inf"</span>))

<span class="c-c"># Top-K Sparsity: keep only top-64 per query row</span>
sparse_res = full(-inf)
sparse_res.scatter_(-<span class="c-n">1</span>, top_k_indices, top_k_values)  <span class="c-c"># zero-mask the rest</span>
attn = softmax(sparse_res)</code></pre>
          </div>
        </div>

        <div class="vis-wrap">
          <canvas id="resCanvas"></canvas>
          <div class="vis-caption">
            Left: binary Q-spike patterns (teal = fired). Top: binary K-spike
            patterns. Center: resonance heatmapâ€‰â€“â€‰brighter = more co-firing
            (stronger connection). The patterns dynamically regenerate to
            simulate continuous processing. The scanner (coral row) processes
            one query position at a time and applies
            <strong>Top-K masking</strong>: only the top-3 strongest connections
            per row survive (highlighted). The rest are killed to âˆ’âˆ before
            softmax, saving 87%+ of memory vs. full attention.
          </div>
        </div>
      </div>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• STEP 5: STDP â•â•â•â• -->
      <div class="card">
        <div class="step-badge">Step 5 Â· Online Learning</div>
        <h2>Reward-Modulated STDPâ€‰â€“â€‰Learning During Inference</h2>
        <p>
          Nord can <strong>update its own weights during inference</strong>,
          even on a phone CPU, using
          <strong>Spike-Timing Dependent Plasticity (STDP)</strong>. The
          biological rule: connections strengthen if a pre-synaptic neuron fires
          just <em>before</em> a post-synaptic neuron (the "Hebbian" or
          causality direction), and weaken in the reverse order.
        </p>
        <p>
          Standard STDP is blind to whether the network is actually improving
          â€‰â€“â€‰it could strengthen weights that produce wrong predictions. Nord
          fixes this with a
          <strong>Reward Signal</strong>: the final weight update is multiplied
          by <code>2 Ã— sigmoid(loss_EMA âˆ’ current_loss) âˆ’ 1</code>, which maps
          to (+1) when predictions are improving (LTP) and (âˆ’1) when they're
          worsening (LTD reversal).
        </p>

        <div class="code-window">
          <div class="code-header">
            <div class="dot"></div>
            <div class="dot"></div>
            <div class="dot"></div>
            <span class="code-fname"
              >nord_core.py Â· STDPEngine.compute_stdp_update() + reward
              modulation</span
            >
          </div>
          <div class="code-body">
            <pre><code><span class="c-k">for</span> t <span class="c-k">in</span> <span class="c-f">range</span>(T):
    <span class="c-c"># Decaying pre- and post-synaptic eligibility traces</span>
    trace_pre  = decay_plus  * trace_pre  + pre_spikes[t]   <span class="c-c"># tau_plus  = 20</span>
    trace_post = decay_minus * trace_post + post_spikes[t]  <span class="c-c"># tau_minus = 20</span>

    <span class="c-c"># LTP: post fires â†’ reinforce connections from active pre neurons</span>
    dW += a_plus  * outer(post_spikes[t], trace_pre)         <span class="c-c"># a_plus  = 0.005</span>
    <span class="c-c"># LTD: pre fires  â†’ weaken connections to recently-active post</span>
    dW -= a_minus * outer(trace_post,     pre_spikes[t])     <span class="c-c"># a_minus = 0.005</span>

<span class="c-c"># Reward modulation: aligns local Hebbian with global LM loss</span>
reward = sigmoid(loss_EMA - current_loss)       <span class="c-c"># 1.0 = improving, 0.0 = worsening</span>
dW_final = dW * (<span class="c-n">2.0</span> * reward - <span class="c-n">1.0</span>)          <span class="c-c"># maps to (âˆ’1, +1)</span>
layer.weight += dW_final.clamp(w_min, w_max)</code></pre>
          </div>
        </div>

        <div class="vis-wrap">
          <canvas id="stdpCanvas"></canvas>
          <div class="vis-caption">
            Four synchronized traces. <strong>Pre-neuron spikes</strong> (coral)
            launch decaying eligibility traces (Track 2).
            <strong>Post-neuron spikes</strong> (teal) launch their own traces
            (Track 3). The <strong>bottom track</strong> maps the final
            reward-modulated Î”W updates: green bars indicate synaptic
            strengthening (LTP), while red bars indicate weakening (LTD). When
            the loss rises, the reward signal flips the update sign, tying local
            plasticity directly to global learning.
          </div>
        </div>
      </div>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• STEP 6: LEAKYCLAMP â•â•â•â• -->
      <div class="card">
        <div class="step-badge">Step 6 Â· Protecting State</div>
        <h2>LeakyClampâ€‰â€“â€‰Keeping the Sub-Threshold Ghost Alive</h2>
        <p>
          In virtually every modern neural network, <strong>ReLU</strong> is the
          non-linearity: for any positive input, pass it through; for any
          negative input, return exactly zero. For a traditional classification
          network this is fine. For a spiking network it is fatal.
        </p>
        <p>
          A LIF neuron's natural resting voltage is
          <code>v_reset = -0.1</code>â€‰â€“â€‰intrinsically negative. ReLU annihilates
          this: the entire sub-threshold state, which carries rich information
          about how close a neuron is to firing, becomes identically zero.
          Gradients cannot flow through zero; neurons can never recover from
          silence.
        </p>
        <p>
          Nord introduces <strong>LeakyClamp</strong>. For positive values it's
          identity; for negative values it applies a
          <strong>learned per-channel leak slope</strong> (â‰ˆ0.1)â€‰â€“â€‰gently
          compressing negative signals down to a
          <strong>learnable floor</strong>
          (initialized at âˆ’0.1). The sub-threshold ghost is preserved.
        </p>

        <div class="code-window">
          <div class="code-header">
            <div class="dot"></div>
            <div class="dot"></div>
            <div class="dot"></div>
            <span class="code-fname">nord_core.py Â· LeakyClamp.forward()</span>
          </div>
          <div class="code-body">
            <pre><code><span class="c-c"># Learnable per-channel parameters (initialized at floor=-0.1, leakâ‰ˆ0.1)</span>
<span class="c-c"># For x â‰¥ 0: identity</span>
<span class="c-c"># For x < 0: compress with leak slope, clamp above floor</span>
neg_part = (self.leak * x).<span class="c-f">clamp</span>(min=self.floor)  <span class="c-c"># leak âˆˆ (0,1)</span>
<span class="c-k">return</span> torch.<span class="c-f">where</span>(x >= <span class="c-n">0</span>, x, neg_part)

<span class="c-c"># Compare to standard ReLU (kills all negative state):</span>
<span class="c-c">#   relu(x) = max(0, x)   â† x = -0.1 â†’ 0.0 (dead!)</span>
<span class="c-c">#   leaky_clamp(-0.1) â†’ 0.1*(-0.1) = -0.01 (preserved!)</span></code></pre>
          </div>
        </div>

        <div class="vis-wrap">
          <canvas id="lkyCanvas"></canvas>
          <div class="vis-caption">
            The input signal (animated dot) represents normalized data sweeping
            from negative to positive.
            <strong>ReLU</strong> (coral line) kills everything below zero,
            permanently destroying the sub-threshold state.
            <strong>LeakyClamp</strong> (teal line) preserves negatives with a
            gentle slope (â‰ˆ0.1Ã—), flooring at the learnable minimum (amber
            dashed line, âˆ’0.1). The ghost of sub-threshold state survives,
            keeping gradients alive across all 60 non-differentiable spike
            barriers.
          </div>
        </div>
      </div>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• STEP 7: EMA â•â•â•â• -->
      <div class="card">
        <div class="step-badge">Step 7 Â· The Final Bottleneck</div>
        <h2>EMA Temporal Readoutâ€‰â€“â€‰From Spikes to Floats</h2>
        <p>
          After 6 deep layers, the network holds 10 timesteps of scattered 1-bit
          binary spikes. How do you accurately predict which of 128,000 English
          words comes next from scattered ones and zeros?
          <strong>You don't.</strong>
        </p>
        <p>
          The final <code>readout_lif</code> passes the spike stream through one
          last LIF step to accumulate its
          <strong>continuous 16-bit membrane potential</strong> tensor
          <code>v_membrane (10 Ã— S Ã— 512)</code>. Rather than doing a simple
          mean over timesteps (which discards temporal ordering), Nord applies
          an <strong>Exponential Moving Average</strong>â€‰â€“â€‰giving the most
          recent timestep the highest weight. The learnable decay
          <code>Î± â‰ˆ 0.8</code> means the contribution of timestep
          <em>t</em> scales as <code>(1âˆ’Î±)Â·Î±^(9âˆ’t)</code>.
        </p>
        <p>
          This hybrid readout (smoothed membrane potential + mean spike rate)
          bypasses the fundamental 1-bit bottleneck that makes SNN language
          modeling "impossible."
        </p>

        <div class="code-window">
          <div class="code-header">
            <div class="dot"></div>
            <div class="dot"></div>
            <div class="dot"></div>
            <span class="code-fname"
              >nord_core.py Â· NordModel.forward()â€‰â€“â€‰EMA readout</span
            >
          </div>
          <div class="code-body">
            <pre><code><span class="c-c"># Readout LIF: spikes AND membrane potential are returned</span>
readout_spikes, v_membrane = readout_lif(x_flat)  <span class="c-c"># (T=10, BÃ—S, D=512)</span>

<span class="c-c"># EMA Temporal Smoothing Readout</span>
alpha = sigmoid(readout_ema_raw)   <span class="c-c"># learnable Î± â‰ˆ 0.80</span>
ema = zeros(B*S, D)
<span class="c-k">for</span> t <span class="c-k">in</span> <span class="c-f">range</span>(T_total):         <span class="c-c"># t=0..9</span>
    ema = alpha * ema + (<span class="c-n">1</span> - alpha) * v_membrane[t]
<span class="c-c"># Weight of v[t] in final ema = (1-Î±)Â·Î±^(9âˆ’t)</span>
<span class="c-c"># t=9 â†’ 0.200 (most recent, highest)   t=0 â†’ 0.027 (oldest, lowest)</span>

<span class="c-c"># Hybrid: smoothed membrane + spike rate â†’ LM head â†’ 128k logits</span>
readout = ema.reshape(B,S,D) + readout_spikes.mean(dim=<span class="c-n">0</span>).reshape(B,S,D)
logits  = lm_head(layernorm(readout))  <span class="c-c"># (B, S, 128256)</span></code></pre>
          </div>
        </div>

        <div class="vis-wrap">
          <canvas id="emaCanvas"></canvas>
          <div class="vis-caption">
            10 membrane potential "frames" (rows, T0â†’T9) each carrying a
            512-dimensional float signal across the horizontal columns. EMA
            weight (shown to the left of each row) grows exponentially:
            <strong>T9 contributes 7Ã— more</strong>
            than T0. The bottom row shows the final collapsed 1-D output vector
            â€‰â€“â€‰still continuous floats, not 1-bit spikesâ€‰â€“â€‰fed directly into the
            128k-vocab language head.
          </div>
        </div>
      </div>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• STEP 8: FULL STACK â•â•â•â• -->
      <div class="card">
        <div class="step-badge">Global Map</div>
        <h2>The Complete Forward Pass</h2>
        <p>
          Everything assembled: one token enters at the top as an integer index.
          It exits at the bottom as a probability distribution over 128,256
          possible next words. Every stage has a dedicated visualization above
          â€‰â€“â€‰this final view shows them all connected, with flowing particles
          representing the data tensor as it transforms through Nord's
          architecture.
        </p>
        <p>
          Notice how the tensor <em>shape</em> changes at each critical stage:
          <code>[1]</code> (token) â†’ <code>[512]</code> (embed) â†’
          <code>[10Ã—512]</code> (temporal) â†’ <code>[10Ã—512]</code> (binary
          spikes, Ã—6 blocks) â†’ <code>[10Ã—512]</code> (membrane) â†’
          <code>[512]</code> (EMA collapse) â†’ <code>[128k]</code> (logits).
        </p>

        <div class="vis-wrap">
          <canvas id="stackCanvas"></canvas>
          <div class="vis-caption">
            Top-to-Bottom vertical pipeline. Watch the dense token embedding
            expand into 10 temporal lanes (teal = fast, amber = slow). At the
            LIF stages, the continuous data becomes
            <strong>97% sparse</strong> across the 6 SNN blocks (only firing
            sporadic coral spikes). Finally, it reconverges via EMA, and expands
            into the 128k-vocab distribution at the bottom.
          </div>
        </div>
      </div>
    </main>

    <footer>
      <div class="container">
        <p>
          Pure Canvas 2Dâ€‰â€“â€‰every animation is a direct simulation of the running
          source code.
        </p>
        <p class="mono" style="margin-top: 10px">
          Project Nord Â· 144M SNN Â· 97% Sparsity Â· $10 Training Cost Â· Built
          from Scratch
        </p>
      </div>
    </footer>

    <script src="script.js"></script>
  </body>
</html>
